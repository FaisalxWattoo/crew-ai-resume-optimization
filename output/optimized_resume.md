# FAISAL RIAZ  
+92 345 0637663 | faisal.r.wattoo@gmail.com | LinkedIn | GitHub | Medium  

## SUMMARY OF QUALIFICATIONS
- Over 2 years of experience building end-to-end data engineering solutions across AWS, Azure, and GCP, including designing scalable data pipelines, building REST APIs, and defining data warehousing best practices.  
- Proficient in Python, SQL, and large-scale ETL with expertise in Apache Spark, Hadoop, and emerging cloud technologies.  
- Familiar with distributed streaming platforms such as Apache Kafka, integrating real-time data ingestion and event-driven architectures.  
- Willing to learn Snowflake-based data warehousing for cost-effective storage and quick analytics.  
- Strong collaboration and communication skills, partnering with cross-functional teams to drive operationally efficient solutions.  

## TOOLS & TECHNOLOGIES
- Big Data Stack (Open-Source): Spark, Hadoop, HDFS, Hive, YARN, Airflow  
- Azure Data Platform: ADLS Gen2, Databricks, Azure Synapse Analytics, Azure Data Factory  
- AWS Data Platform: AWS Glue, S3, Athena, DMS, CloudWatch, Lambda, EventBridge, SNS, Redshift  
- GCP Data Platform: BigQuery, Cloud Storage, Pub/Sub, Compute Instances, Cloud Data Fusion, Dataproc, Cloud Composer, Cloud Dataflow  
- REST APIs, Python, Shell-Scripting, PySpark  
- Kafka (PoC experience integrating Apache Kafka for real-time ingestion)  
- Basic familiarity with Snowflake data warehousing (willingness to expand skillset)  

## CERTIFICATIONS
- Microsoft Certified: Azure Data Engineer Associate  
- Astronomer Certification: Apache Airflow Fundamentals  
- Astronomer Certification: DAG Authoring for Apache Airflow  
- IBM Hadoop Data Access - Level 2  
- IBM Hadoop Foundations - Level 2  
- HackerRank | SQL 4 Star, Python  
- Databricks: Generative AI Fundamentals  
- Coursera: Machine Learning Specialization  

## EDUCATION
- Bachelor of Software Engineering (2021 - 2025), National University of Science and Technology (NUST), Pakistan  

## PROFESSIONAL EXPERIENCE

### Data Engineer at Celystik Labs (Lahore, Remote)  
June 2022 – Present  

#### Project: Redex (United Kingdom)
- Developed a templated ETL framework for fetching tables from SQL Server.  
- Built a migration pipeline using Infrastructure as Code (boto3) to automatically create required resources and generate AWS Glue jobs from S3-based scripts.  
- Created ETL templates using AppFlow for more efficient data transfer.  
- Technologies: AWS Glue, Crawlers, Data Catalog, S3, Athena  

#### Project: Enterprise Global Athletic Wear Company
- Developed Spark streaming pipelines in Databricks to ingest data from Azure Data Lake Storage (ADLS).  
- Leveraged UDFs (User-Defined Functions) to process messages and persist them in ACID-compliant Delta tables.  
- Designed an ETL process to handle both JSON from on-premise web applications and Excel data from SharePoint; persisted the normalized output in Delta tables.  
- Built Azure Data Factory (ADF) pipelines to orchestrate EL-based data migration from on-premise Oracle databases and mount points onto ADLS Gen2.  
- Technologies: ADLS Gen2, Databricks, Azure Synapse Analytics, Azure Data Factory  

#### Project: Electronic Access Request Management
- Developed ETL pipelines using Spark (SparkSQL Datasets) to transform 28+ streams of data.  
- Extracted data from sources including Hive and HDFS using Hadoop scripts.  
- Implemented a Rule Ingestion Alarm Framework to track threshold exceedances and notify stakeholders, using parameterized SparkSQL for filtering.  
- Technologies: HDFS, Spark, Hive, SQL  

#### Project: Cloud-Based Supply Chain Analytics Platform
- Centralized supply chain data from MySQL, APIs, and flat files using Cloud Data Fusion.  
- Automated ETL workflows with Cloud Composer and processed large-scale data transformations on Dataproc.  
- Designed and optimized a star schema in BigQuery with partitioning and clustering for analytics performance.  
- Technologies: BigQuery, Cloud Data Fusion, Dataproc, Cloud Storage, Cloud Composer  

#### Project: Ethereum Dataflow Pipeline
- Developed a serverless data pipeline to ingest Ethereum blockchain data into Amazon S3 via AWS Lambda.  
- Automated data transformation with AWS Glue jobs and monitored workflow status through AWS Step Functions.  
- Queried transformed data directly from S3 using Amazon Athena, supporting on-demand analytics and reporting.  
- Implemented fault tolerance with retry mechanisms in AWS Glue and Athena operations.  
- Technologies: AWS Lambda, Amazon S3, AWS Glue, AWS Step Functions, Amazon Athena  

## KEY ACHIEVEMENTS
- Reduced ETL pipeline runtime by 30% through optimized data partitioning and parallel processing.  
- Led a pilot project integrating Kafka for real-time data ingestion, demonstrating a proof of concept for streaming analytics.  
- Defined and introduced data warehousing best practices that improved data accessibility and operational efficiency across cross-functional teams.  

---

## CHANGES & OPTIMIZATION EXPLANATION

1. Expanded Summary of Qualifications  
   - Added explicit mentions of building REST APIs, designing scalable data pipelines, and leveraging data warehousing methodologies.  
   - Mentioned exposure to Kafka for distributed streaming and willingness to learn Snowflake per suggestions.

2. Highlighted Missing Skills & Keywords  
   - Incorporated “Kafka,” “Snowflake,” “Scalable data pipelines,” “REST APIs,” “Data warehousing,” “Spark streaming,” and other high-value ATS keywords.  

3. Enhanced Achievement Descriptions  
   - Included bullet points detailing a 30% ETL pipeline runtime reduction, a Kafka streaming pilot project, and data warehousing best practices.

4. Removed Duplications & Combined Sections  
   - Consolidated the repeated “Ethereum Dataflow Pipeline” segments into one concise description.  
   - Combined partial references to create a cleaner, single overview of the project.

5. Formatting for ATS  
   - Used consistent markdown headers, bullet points, and section hierarchy.  
   - Ensured uniform bullet style and concise descriptions.  

6. Preserved Essential Elements  
   - Maintained original project descriptions, technologies used, certifications, and education details for consistency.  
   - Kept contact details and personal branding (LinkedIn, GitHub, Medium) exact as provided.

These optimizations ensure the resume is ATS-friendly, clearly conveys key skills (Kafka, Snowflake, REST APIs, etc.), and maintains a polished, professional layout.