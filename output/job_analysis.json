{"technical_skills":["Python","AWS","GCP","Azure","Spark","Hadoop","ETL","SQL","Data Warehousing","REST APIs","SQL DB Administration","Airflow","Glue","Athena","BigQuery"],"soft_skills":["Collaboration","Communication","Cross-functional teamwork"],"experience_requirements":["2+ years of software/data engineering experience with Python","Experience creating data pipelines and ETL with large datasets","Proficiency working with cloud data warehousing technologies","Ability to build REST APIs"],"key_responsibilities":["Design database and data pipelines/ETL using emerging technologies","Drive operationally efficient analytic solutions","Define standards and methodologies for data warehousing","Design and build scalable data pipelines (AWS, Snowflake, Spark, Kafka)","Translate complex business requirements into technical solutions","Create scalable ETL applications that support business operations","Assist with data migration issues and system performance improvements","Collaborate with product management, technical program management, operations, and other engineers"],"education_requirements":["BS, MS, or Ph.D. in Computer Science or a relevant technical field"],"nice_to_have":["Understanding of complex, distributed, microservice web architectures","Python back-end and ETL to move data across databases","Solid understanding of analytics"],"job_title":"Remote Data Engineer","department":null,"reporting_structure":null,"job_level":null,"location_requirements":{"location_type":"Remote","timezone_overlap":"4 hours overlap with U.S. time zones"},"work_schedule":null,"travel_requirements":null,"compensation":{"salary":"Not specified","currency":"USD"},"benefits":[],"tools_and_technologies":["AWS","GCP","Azure","Snowflake","Spark","Kafka","Airflow","Glue","Redshift","BigQuery","SQL","Python"],"industry_knowledge":["Big data engineering","Data analytics","Cloud-based data processing"],"certifications_required":[],"security_clearance":null,"team_size":null,"key_projects":[],"cross_functional_interactions":["Collaboration with product management, technical program management, operations, and engineering teams"],"career_growth":["Opportunities to work with top-tier companies and challenging projects","Potential for long-term growth and skill development"],"training_provided":[],"diversity_inclusion":null,"company_values":[],"job_url":"https://www.turing.com/jobs/remote-data-engineer","posting_date":null,"application_deadline":null,"special_instructions":[],"match_score":{"overall_match":0.84,"technical_skills_match":0.9,"soft_skills_match":0.85,"experience_match":0.8,"education_match":0.7,"industry_match":0.8,"skill_details":[{"skill_name":"Python","required":true,"match_level":1.0,"years_experience":2.0,"context_score":1.0},{"skill_name":"AWS","required":true,"match_level":1.0,"years_experience":2.0,"context_score":1.0},{"skill_name":"Spark","required":true,"match_level":0.9,"years_experience":2.0,"context_score":0.9},{"skill_name":"Kafka","required":false,"match_level":0.0,"years_experience":null,"context_score":0.0},{"skill_name":"Data Warehousing","required":true,"match_level":0.9,"years_experience":2.0,"context_score":0.9},{"skill_name":"SQL DB Administration","required":true,"match_level":0.8,"years_experience":1.5,"context_score":0.8}],"strengths":["Extensive knowledge of AWS, GCP, and Azure","Proficiency with Spark and Hadoop ecosystem","Hands-on experience with big data pipelines and ETL processes"],"gaps":["Direct experience with Kafka not mentioned","No direct mention of Snowflake","Bachelor's degree still in progress"],"scoring_factors":{"technical_skills_weight":0.35,"soft_skills_weight":0.2,"experience_weight":0.25,"education_weight":0.1,"industry_weight":0.1}},"score_explanation":["Candidate meets required years of experience for data engineering, with strong exposure to multiple cloud platforms","Python, SQL, and Spark proficiency strongly align with job requirements","Bachelor's degree is still in progress which slightly lowers education match","No explicit Kafka or Snowflake experience mentioned in resume"]}